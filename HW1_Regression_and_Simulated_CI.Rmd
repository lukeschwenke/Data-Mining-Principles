---
title: 'Assignment #1: German Credit'
author: "Luke Schwenke"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(dplyr)
library(ggplot2)

data(GermanCredit)
```

# Regression Model

```{r model, echo=TRUE, warning=FALSE}

df <- GermanCredit
df <- df %>% dplyr::select(-Class) # Remove Class variable

full_model <- lm(Amount ~ ., data=df)

# Save and Print Coefficients
full_model_coeff <- full_model$coefficients
full_model_coeff

```

# Split into Train/Test Sets and Run Model 1,000 times

```{r set_seed, echo=TRUE, warning=FALSE}

set.seed(777)

# Create ID to help with splitting into Train/Test
df$id <- 1:nrow(df)

# Initialize empty Data Frame
my_df <- data.frame()

# Run 1,000 Linear models
for(n in 1:1000){
  
  # Split 63.20% of the data into the train set and the rest into the test set
  train <- df %>% dplyr::sample_frac(0.632)
  test  <- dplyr::anti_join(df, train, by = 'id') 
  
  # Drop id column before running model since it does not have value
  train$id <- NULL
  test$id  <- NULL

  # Run the linear model on all the independent variables
  model <- lm(Amount ~., data=train)
  
  # Capture the predictions on the Test / Holdout set
  predictions <- predict(model, test)
  
  # Save the coefficients, and R-squared for the training and holdout
  save_coeff <- model$coefficients
  save_r2_training <- summary(model)$r.squared
  save_r2_holdout <- cor(test$Amount, predict(model, newdata = test))^2
  
  con <- c(save_coeff, save_r2_training, save_r2_holdout)
  my_df <- rbind(my_df, con)
}

# Remove ID variable and Amount (dependent) variable
df$id <- NULL
df$Amount <- NULL

# Update Column Names
colnames(my_df)[1] <- "(Intercept)"
colnames(my_df)[2:61] <- names(df)
colnames(my_df)[62] <- "training_r2"
colnames(my_df)[63] <- "holdout_r2"

head(my_df, 3)

```

## Coefficient & R-Squared Distributions

```{r coeff distributions - Age}

hist(my_df$Age, 
     breaks=30,
     xlab = "Coefficients", main = "Coefficient Distribution - Age")

```


```{r coeff distributions - Residence Duration, echo=TRUE}
hist(my_df$ResidenceDuration, 
     breaks=30,
     xlab = "Coefficients", main = "Coefficient Distribution - Residence Duration", col="lightblue")
```


```{r coeff distributions - Housing Rent, echo=TRUE}
hist(my_df$Housing.Rent, 
     breaks=30,
     xlab = "Coefficients", main = "Coefficient Distribution - Housing Rent", col="lightgreen")
```

## Summary - Interpretation of Above Plots ~ all plots printed above are fairly normally distributed. See individual interpretations here:

### Age: the age coefficients are sometimes negatives which is the opposite of what I would normally think. Generally older people have higher credit, so seeing that the model sometimes says a higher age means a lesser loan is surprising. Overall the plot indicates age tends to have a positive impact on the loan amount, i.e., older people get higher loans.

### Residence Duration: this coefficient value is mostly negative meaning the longer someone resides in a location, the lower their predicted loan Amount. This may be because wealthier people who can get higher loans tend to move around more and can afford to move to new places. This matches expectations.

### Housing Rent: this coefficient value is mostly positive meaning the higher someone's rent the higher their predicted loan amount tends to be. This makes sense is people who can afford more expensive living places will generally be approved for higher loans due to high credit scores and proof of income.

```{r distributions - Training R2, echo=TRUE, warning=FALSE}
hist(my_df$training_r2, 
     breaks=30,
     xlab = "Training R-Squared Values", main = "Distribution of Training R-Squared", col="pink")

mean(my_df$training_r2)
median(my_df$training_r2)
```

```{r distributions - R2 difference, echo=TRUE, warning=FALSE}
my_df <- my_df %>% mutate(r2_decrease = (training_r2 - holdout_r2)/training_r2)
head(my_df$r2_decrease, 5)

hist(my_df$r2_decrease, 
     breaks=12,
     xlab = "Decrease Values", main = "Distribution of R-Squared Decrease (Training vs. Holdout)",    
     col="lightyellow")

mean(my_df$r2_decrease)
median(my_df$r2_decrease)

print(my_df %>% dplyr::select(training_r2, holdout_r2, r2_decrease) %>% head(50))
```

## Interpretation:
### These plots are also normally distributed and indicate that on average we can expect around a 12% percentage decrease from our training dataset R-squared of 62% to our holdout dataset. This means the model performs slightly worse on the holdout data. This is to be expected because this is unseen data -- we just want the model to be able to generalize well, not necessarily have an excellent R-squared value. This is a good result.


```{r means and sds, echo=TRUE, warning=FALSE}

coefficient_means <- colMeans(my_df)
head(coefficient_means, 5)

coefficient_sds <- sapply(my_df, sd)
head(coefficient_sds, 5)


# Difference between coefficient_means and actual model coefficients
bind <- data.frame(cbind(coefficient_means, save_coeff))
bind$abs_raw_diff <- abs(coefficient_means - save_coeff)
bind$percent_diff <- 100*abs(((coefficient_means - save_coeff) / save_coeff))

print(bind %>% na.omit())
```

## Calculate Confidence Intervals & Width
```{r CI, echo=TRUE, warning=FALSE}

# Confidence Interval for Rep 1,000 Coefficients -------------------------------
# Transposed dataframe of confidence intervals for each variable
rep_conf <- data.frame(t(sapply(my_df[,1:63], function(x) Rmisc::CI(x, ci=0.95)))) #%>% na.omit()

# Calculate Width
# rep_conf$width <- (rep_conf$upper-rep_conf$lower)*sqrt(0.632)
rep_conf$width <- (rep_conf$upper-rep_conf$lower)*sqrt(0.632)

# MANUAL CHECK =============================
# Calculate Means
#means <- data.frame(means=sapply(my_df[1:61], function(x) mean(x)))
#n <- 1000
# Calculate Standard Deviation
#std_dev <- data.frame(std_dev=sapply(my_df[1:61], function(x) sd(x)))

#std_error <- std_dev / sqrt(n)
#alpha = 0.025
#degrees_of_freedom = n-1
#t_score = qt(p=alpha/2, df=degrees_of_freedom,lower.tail=F)
#margin_error <- t_score * std_error

#lower_bound_new <- means - margin_error
#upper_bound_new <- means + margin_error

#x <- cbind(lower_bound_new, upper_bound_new) %>% dplyr::rename(lower_new = 1, upper_new = 2)

#margin <- qnorm(0.975)*std
#rep_CI_low_manual <- means - margin
#rep_CI_high_manual <- means + margin
#manual <- cbind(rep_CI_low_manual, rep_CI_high_manual) %>% 
#                dplyr::rename(lower_manual = 1, upper_manual = 2)


# =============================

# Reorder columns
rep_conf <- rep_conf %>% select(lower, upper, width)

# Keep only valid columns, remove R2 values
rep_conf <- rep_conf[1:61,]

# COMBINE for check
#z <- cbind(rep_conf, manual)
#z$width_manual <- z$upper_manual - z$lower_manual*sqrt(0.632)
#View(z)

# Check row count after ommitting NA
nrow(rep_conf)

# Confidence Interval for Full Model -------------------------------------------
# Calculate confidence interval and rename the columns
full_model_conf <- data.frame(confint(full_model, level=0.975)) %>% 
                   #na.omit() %>% 
                   dplyr::rename(lower_full = 1, upper_full = 2)

# Calculate Width
full_model_conf <- full_model_conf %>% 
                   #mutate(index=1:nrow(full_model_conf)) %>% 
                   #filter(index %in% (2:48)) %>% 
                   mutate(width_full=upper_full-lower_full)

# Check row count
nrow(full_model_conf)

#t <- cbind(rep_conf, full_model_conf)

row.names(rep_conf) == row.names(full_model_conf)

#View(cbind(row.names(rep_conf), row.names(full_model_conf)))

```
## Determine how many of the repeated sample CI's are tighter
```{r Final Calcs, echo=TRUE, warning=FALSE}

# Combine the 2 dataframes and remove NA values
calc <- cbind(rep_conf, full_model_conf) %>% na.omit()

#Calculate how many of the repeated sample CI’s are tighter or broader than the full model CI’s. If the #width is smaller, the CI is tighter. If the width is bigger, the CI is broader.

# 1 means the CI is tighter (width of repeated is smaller)
calc$tighter_CI_flag <- ifelse(calc$width < calc$width_full, 1, 0)

print(calc)

```
``` {r count, echo=TRUE, warning=FALSE}
percent <- sum(calc$tighter_CI_flag)/nrow(calc)
print(percent*100)

sum(calc$tighter_CI_flag)

```


## Conclusion: 100% of the simulated/repeated model's CI's are tighter/smaller. In other words, of the 47 columns left after removing NA's and the class column, 47 of the columns' confidence intervals from the repeated samples are smaller/tighter than those comapred to the CI's from the full model.

## This tells us that when we repeat the model 1,000 times, the confidence in the best coefficients for each independent variable has increased (the range of our confidence intervals are smaller). Intuitively this makes sense because instead of just running 1 model and trying to fit the best coefficients, we have now done this 1,000 times and taken averages to make this determination. If we were to sample 10,000 times, the confidence intervals would be even tighter. There should be convergence at some point (diminishing returns), but there will be an improvement event passed 1,000.

## In many instances, the repeated model's coefficient means were quite different from the single model we saved the coefficients for. The median difference between the repeated model mean and the corresponding saved model coefficients was 21.5%. This tells us that running just 1 model does not accurately capture what we can assume would be a "better" coefficient to use -- models should be run multiples time to find the optimal fit.
